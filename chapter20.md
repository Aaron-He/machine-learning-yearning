## 20. 偏差（Bias）和方差（Variance）：错误的两大来源

假设你的训练集、开发集和测试集全部都来自于同一分布。那么你一你应该总是尝试获得更多的训练数据，因为只有那样才能提高性能，对吗？

尽管拥有更多的数据没有什么妨害，但很不幸的是，它并不总能如你所想的那样有所帮助。努力获取更多数据可能是浪费时间。那么，如何决定什么时候去增加数据，什么时候不增加呢？

机器学习里存在两大错误来源：偏差和方法。了解它们将有助于你决定是否添加数据及其他能提高性能的策略，是对时间的一种很好的利用。

假设你希望构建一个有5%错误率的猫识别器。目前，你的训练集错误率是15%，并且你的开发集错误率是16%。在这种情况下，添加更多的数据可能无济于事。你应该关注其他变化。事实上，在你的训练集上增加更多的数据只会让你的算法更难以在训练集上做的更好。（我们会在后续的章节解释为什么）

如果你的训练集错误率是15%（或者说85%的准确率），但是你的目标是5%错误率（或者说95%的准确率），那么要解决的第一个问题是提高算法在训练集上的表现。你的开发集、测试集上性能通常比你的训练集性能差。所以如果你的算法在它看过的训练集上的准确率达到85%，那么你的算法在它没有看过的样本上不会达到95%。

假设如上所述，你的算法在开发集上有16%的错误率（84%的准确率）。我们将16%的错误率分成两部分：

首先，你的算法在训练集上的错误率。在这个例子中，它是15%。我们非正式的将此认为是**偏差（Bias）**。

其次，算法在开发集（或测试集）上比训练集上差多少。在这里例子中，它在开发集上比训练集上差1%。我们非正式的将此认为是算法的**方差（variance）**[1]。

一些改变可以帮助学习算法改变第一个组成部分的错误——**偏差**——并提高其在训练集上的性能。一些更改可以帮助解决算法的第二个组成部分——**方差**——并帮助它从训练集上推广到开发集/测试集。要选择最有希望的更改，了解这两个错误那个一个耿金波，是非常有用的。

开发关于偏差和方差的良好直觉将帮助你为你的算法选择更有效的更改。

—————————————————————————————

[1]. 统计学里有更多正式的偏差和方差的定义，我们不用担心。粗略的说，当你有一个非常大的训练集，偏差是算法在训练集上的错误率。与训练集相比，方差是你的算法在测试集上做的更差多少。当你的误差度量是均方误差（mean squared error）时，你可以写下这两个量的公式，并证明总误差（Total Error）=偏差（Bias）+ 方差（Variance）。但为了决定如何在ML问题上取得进展的目的，这里给出的更偏非正式的偏差和方差的定义就足够了。

[2]. 还一些方法可以通过对系统框架进行重大改变同时减少偏差和方差。但这些往往难以识别和实施。














