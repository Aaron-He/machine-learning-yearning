## 22. 对比最优错误率

在我们的猫识别示例中，“理想”错误率——即“最优”分类器可以实现错误率几乎0%。一个看着照片的人几乎总是能够识别它是否包含一只猫，因此我们希望能够一个可以做到这一点的机器。

其他的问题更难。例如，假设你正在建立一个语音识别系统，并且发现14%的音频片段有很多的背景噪声或太难以理解以至于人都无法理解所说的内容。在这种情况下，即使最“最优”的语音识别系统可能也有14%左右误差。

假设在这个语音识别系统上，你的算法实现了：

- 训练误差=15%
- 开发误差=30%

训练集的性能已经接近最优错误率14%。因此，就偏差或训练集的性能而言，已经没有太大提升空间。但是这种算法对于开发集没有很好地通用性，所以由于方差而存在很大的改进空间。

这个例子类似于上一章的第三个例子，它的训练错误率为15%，开发错误率是30%。如果最优错误率是约等于0%，那么15%的训练误差留有很大的改进空间。这表明减少偏差的变化可能是有益的。但是如果最有错误率是14%，那么相同的训练集表现告诉我们，分类器的偏差几乎没有改进的余地。

对于算法最有错误率不为0的问题，下面是算法错误更详细的分类。继续上面的语音识别的例子，可以将30%的总开发集误差分解如下（类似的分析适用于测试集误差）：

- **最优错误率（“不可避免的偏差”）（Optimal error rate (“unavoidable bias”)）**：14%。假设我们认定，即使世界上最好的语音系统，我们仍然会遭受14%的错误。我们可以将其视为学习算法“不可避免”的偏差。

- **可以避免的偏差（Avoidable bias）**：1%。这是计算训练误差和最优误差之间的差值来计算的[3]。

- **方差（Variance）**：15%。开发误差和训练误差之间差值。

为了将我们早先的定义联系起来，偏差和可以避免的偏差的关系如下[4]：

偏差 = 最优误差率（“不可避免的偏差”）+可以避免的偏差

这“可避免的偏差”反映了算法在训练集上表现比“最优分类器”差多少。

方差的概念和以前一样。从理论上讲，我们可以通过训练一个大规模的训练集来使方差接近于0.因此所有的方差都是“可避免的”，而且数据集足够大，所以不存在“不可避免的方差“的问题。

再考虑一个例子，最有错误率是14%，我们有：

- 训练误差=15%
- 开发误差=16%

而在前一章中，我们称之为高偏差分类器，现在我们可以说可避免偏差的误差为1%，方差的误差约为1%。因此，该算法已经很好，几乎没有改进的余地。它金币最优错误率差2%。

从这些例子中我们可以看出，知道最优错误率有助于指导我们后续的步骤。在统计中，最优错误率也被称为**贝叶斯错误率（Bayes error rate）**或贝叶斯率。

我们如何知道最有错误率是多少？对于人类合理擅长的任务，例如识别图片或标注音频片段，你可以要求人类提供标签，然后测量人类的标签相对于训练集的准确性。这将给出最优错误率的估计。如果你正在解决即使对人类来说也是困难的事情，比如预测要推荐什么电影，或向用户展示什么广告，那可能很难估计最优错误率。

在“与人类的表现比较（第33-35章）”一节中，我将更加详细地比较学习算法表现与人类表现的过程。

在最近几章中，你学习了如何通过查看训练集和测试集误差估计可以避免的/不可避免的偏差和方差。下一章节将讨论如何使用这种分析的简介来决定优先考虑减少偏差还是优先考虑减少方差。根据项目目前的问题是高（可以避免的）偏差还是高方差，你应该使用非常布偶听得技术。请继续阅读！

—————————————————————————————

[3]. 如果这个数字是负数，你在训练集上的表现比最优错误率好。这意味着你对训练集过度拟合，并且该算法过度记忆了训练集。你应该关注减少方差的方法，而不是进一步减少偏差。

[4]. 这些定义为了传达关于如何改进学习算法的见解。这些定义与统计学家关于偏差和方差的定义不同。从技术上讲，我在这里定义的“偏差”应该被称为“因为偏差带来的错误”，和“可以避免的偏差”应该是我们的学习算法比最优错误率超出的部分。












